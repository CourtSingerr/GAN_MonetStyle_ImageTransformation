{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPlK0SEOSoy89gCVxv9m4ry",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CourtSingerr/GAN_MonetStyle_ImageTransformation/blob/main/GANsKaggleProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GANs Kaggle Mini Project\n",
        "#### Kaggle Competition: I’m Something of a Painter Myself - Use GANs to create art - will you be the next Monet?\n",
        "#### https://www.kaggle.com/competitions/gan-getting-started\n",
        "##### Courtney Singer, November 2024\n",
        "\n"
      ],
      "metadata": {
        "id": "M3p_6HfPVu6-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "5DOsCnGOWOLc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Description of the Problem\n",
        "\n",
        "The core task in this competition is to learn a transformation from one domain of images (real photographs) to another (Monet-style paintings).\n",
        "\n",
        "Instead of having pairs of images showing the same scene in both styles, this challenge relies on unpaired image-to-image translation— an ideal scenario for applying a Generative Adversarial Network (GAN) architecture.\n",
        "\n",
        "**The goal** is to generate convincing Monet-style artwork from ordinary Photographs.\n",
        "\n",
        "In this analysis, I compare two architectures - CycleGAN and Contrastive Unpaired Translation. CycleGAN proves to be more effective and is used in my final model."
      ],
      "metadata": {
        "id": "wS_wofsE8ZKi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "SDVd9dxENg6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview of the Data\n",
        "\n",
        "The dataset consists of two distinct image collections: a relatively small set of approximately 300 Monet paintings and a larger set of about 7,000 real landscape photographs.\n",
        "\n",
        "**Source:** The dataset contains two distinct sets of images: one comprised of Monet paintings and the other composed of real landscape photographs.\n",
        "\n",
        "**Size and Composition:**\n",
        "  - Monet Paintings: Approximately 300 images of Monet-style artwork\n",
        "  - Real Photographs: Around 7,000 images of everyday scenes and landscapes\n",
        "  - Image dimensions:generally provided at a resolution of 256x256 pixels\n",
        "  - All images are in RGB format, giving three color channels.\n"
      ],
      "metadata": {
        "id": "mcDzEFO_8dng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Data\n",
        "\n",
        "I imported the datasets directly from Kaggle using the Kaggle API and then extracted the files in your Colab environment."
      ],
      "metadata": {
        "id": "j2SzNgX68311"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install kaggle\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "yczXEeNW9Qqh",
        "outputId": "d9029660-ecfc-44cf-9a55-96b637959d48"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.3.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.11/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle) (3.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-CgGsfwt84Bi",
        "outputId": "9965400b-da48-46a0-837d-424365edd0ef"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!kaggle competitions download -c gan-getting-started -p ./data\n",
        "!unzip ./data/gan-getting-started.zip -d ./data"
      ],
      "metadata": {
        "id": "wnUkcUi59hqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "taHbJJQ7Ni0p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "obzi3WnA8-nb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploration of Raw Data\n",
        "\n",
        "**Basic Directory and File Checks**"
      ],
      "metadata": {
        "id": "idAxh9gQIsSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Paths (adjust if you extracted to a different location)\n",
        "data_path = '/content/data'  # This should be where you unzipped your data\n",
        "monet_dir = os.path.join(data_path, 'monet_jpg')\n",
        "photo_dir = os.path.join(data_path, 'photo_jpg')\n",
        "\n",
        "# Count the number of images in each directory\n",
        "monet_images = os.listdir(monet_dir)\n",
        "photo_images = os.listdir(photo_dir)\n",
        "\n",
        "print(f\"Number of Monet images: {len(monet_images)}\")\n",
        "print(f\"Number of Photo images: {len(photo_images)}\")"
      ],
      "metadata": {
        "id": "OZmfHtaUIsjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualizing a Few Samples**\n",
        "\n",
        "In this step I view a handful of images from each set to get a qualitative feel for the data."
      ],
      "metadata": {
        "id": "HJ74vhtwIxke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import random\n",
        "\n",
        "# Function to show a grid of images\n",
        "def show_images(image_paths, title, num=5):\n",
        "    plt.figure(figsize=(15,3))\n",
        "    for i in range(num):\n",
        "        img_path = random.choice(image_paths)\n",
        "        img = mpimg.imread(os.path.join(data_path, img_path))\n",
        "        plt.subplot(1, num, i+1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(title)\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "show_images([f\"monet_jpg/{f}\" for f in monet_images], title=\"Monet Paintings\")\n",
        "show_images([f\"photo_jpg/{f}\" for f in photo_images], title=\"Real Photos\")"
      ],
      "metadata": {
        "id": "MEeLmgLFIxtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Image Dimensions & Aspect Ratios**\n",
        "\n",
        "Check if all images are consistent in dimensions and aspect ratios. You can also quickly verify image modes or any anomalies."
      ],
      "metadata": {
        "id": "vqe9aBDRI4gc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def get_image_stats(image_dir, num_check=100):\n",
        "    widths, heights = [], []\n",
        "    image_files = os.listdir(image_dir)\n",
        "    sample_files = image_files[:num_check]  # check only a subset for speed\n",
        "    for f in sample_files:\n",
        "        img_path = os.path.join(image_dir, f)\n",
        "        img = Image.open(img_path)\n",
        "        w, h = img.size\n",
        "        widths.append(w)\n",
        "        heights.append(h)\n",
        "    return widths, heights\n",
        "\n",
        "monet_widths, monet_heights = get_image_stats(monet_dir)\n",
        "photo_widths, photo_heights = get_image_stats(photo_dir)\n",
        "\n",
        "print(f\"Monet Images: mean width={np.mean(monet_widths):.2f}, mean height={np.mean(monet_heights):.2f}\")\n",
        "print(f\"Photo Images: mean width={np.mean(photo_widths):.2f}, mean height={np.mean(photo_heights):.2f}\")"
      ],
      "metadata": {
        "id": "bsgISLjII4zN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Color Channel Distributions**\n",
        "\n",
        "It could be interesting to see the distribution of pixel intensities for Monet paintings vs. real photos. For instance, check mean and standard deviation of pixel values across a sample:\n",
        "\n",
        "This can give you insight into how Monet paintings differ in terms of color distribution compared to actual photos."
      ],
      "metadata": {
        "id": "0KEvJ6KxJCN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def load_and_preprocess(img_path):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)  # Scale [0,1]\n",
        "    return img\n",
        "\n",
        "def image_pixel_stats(image_dir, num_check=100):\n",
        "    image_files = os.listdir(image_dir)[:num_check]\n",
        "    means = []\n",
        "    stds = []\n",
        "    for f in image_files:\n",
        "        img_path = os.path.join(image_dir, f)\n",
        "        img = load_and_preprocess(img_path)\n",
        "        means.append(tf.reduce_mean(img, axis=[0,1]))\n",
        "        stds.append(tf.math.reduce_std(img, axis=[0,1]))\n",
        "    mean = tf.reduce_mean(tf.stack(means), axis=0)\n",
        "    std = tf.reduce_mean(tf.stack(stds), axis=0)\n",
        "    return mean, std\n",
        "\n",
        "monet_mean, monet_std = image_pixel_stats(monet_dir)\n",
        "photo_mean, photo_std = image_pixel_stats(photo_dir)\n",
        "\n",
        "print(\"Monet mean RGB:\", monet_mean.numpy())\n",
        "print(\"Monet std RGB:\", monet_std.numpy())\n",
        "print(\"Photo mean RGB:\", photo_mean.numpy())\n",
        "print(\"Photo std RGB:\", photo_std.numpy())"
      ],
      "metadata": {
        "id": "W4L2vFHDJCUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Simple Histogram Visualization for Color Channels**\n",
        "\n",
        "Plot histograms of pixel intensities for a few samples to visually inspect color distributions."
      ],
      "metadata": {
        "id": "hxMInD0mJREc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_image_histogram(img_path):\n",
        "    img = load_and_preprocess(img_path).numpy()\n",
        "    colors = ['r', 'g', 'b']\n",
        "    plt.figure(figsize=(10,3))\n",
        "    for i, c in enumerate(colors):\n",
        "        plt.hist(img[:,:,i].flatten(), bins=50, alpha=0.5, color=c)\n",
        "    plt.title(\"Color Channel Distribution\")\n",
        "    plt.xlabel(\"Pixel Intensity\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.show()\n",
        "\n",
        "# Example: Plot histogram for one Monet image and one Photo image\n",
        "monet_sample = os.path.join(monet_dir, random.choice(monet_images))\n",
        "photo_sample = os.path.join(photo_dir, random.choice(photo_images))\n",
        "\n",
        "print(\"Monet image histogram:\")\n",
        "plot_image_histogram(monet_sample)\n",
        "\n",
        "print(\"Photo image histogram:\")\n",
        "plot_image_histogram(photo_sample)"
      ],
      "metadata": {
        "id": "5_vs4FeMJRLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Summary and Observations\n",
        "\n",
        "After running these EDA steps, you might note:\n",
        "- The number of Monet images is much smaller than the number of photos.\n",
        "- Image sizes might be consistent (256x256) or may need resizing.\n",
        "- Monet paintings have different color distributions (more pastel tones, etc.) compared to the real photos.\n",
        "- Pixel intensity distributions and mean values differ between Monet and real photos, which is what the model will need to learn to transform.\n",
        "\n",
        "These insights can guide how you preprocess the data, choose normalization techniques, and set up your training pipeline."
      ],
      "metadata": {
        "id": "LDxT7pR5JZdS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Cleaning\n",
        "\n",
        "The dataset for the “GAN - Getting Started” competition is relatively clean since it’s curated for a specific challenge.\n",
        "\n"
      ],
      "metadata": {
        "id": "Hu1F1zziJwTu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check for Corrupted or Incomplete Images**\n",
        "\n",
        "Occasionally, data downloads can be incomplete or corrupted. A good sanity check:"
      ],
      "metadata": {
        "id": "y4IiRr_mJ9Ag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "def verify_images(directory):\n",
        "    for fname in os.listdir(directory):\n",
        "        fpath = os.path.join(directory, fname)\n",
        "        try:\n",
        "            with Image.open(fpath) as img:\n",
        "                img.verify()  # just verify if it can be opened\n",
        "        except (IOError, SyntaxError) as e:\n",
        "            print(f\"Corrupted file found: {fpath}\")\n",
        "\n",
        "# Check both Monet and Photo directories\n",
        "verify_images('/content/data/monet_jpg')\n",
        "verify_images('/content/data/photo_jpg')"
      ],
      "metadata": {
        "id": "zF2TowNvKCu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ensure Consistent Dimensions**\n",
        "\n",
        "While most images should already be uniformly sized (often 256x256), verify this. If any images differ, resize them:"
      ],
      "metadata": {
        "id": "9CE3CA4IKGC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "IMG_SIZE = 256\n",
        "\n",
        "def resize_and_overwrite(directory):\n",
        "    for fname in os.listdir(directory):\n",
        "        fpath = os.path.join(directory, fname)\n",
        "        img = cv2.imread(fpath)\n",
        "        if img is not None and (img.shape[0] != IMG_SIZE or img.shape[1] != IMG_SIZE):\n",
        "            img_resized = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "            cv2.imwrite(fpath, img_resized)\n",
        "\n",
        "resize_and_overwrite('/content/data/monet_jpg')\n",
        "resize_and_overwrite('/content/data/photo_jpg')"
      ],
      "metadata": {
        "id": "W74MiBWUKN-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remove Duplicate or Near-Duplicate Images**\n",
        "\n",
        "While not strictly necessary, you could check for duplicates. Near-duplicates could bias the model. This step is optional and more involved:"
      ],
      "metadata": {
        "id": "I2H7DQ9bKQ5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install imagehash\n",
        "from PIL import Image\n",
        "import imagehash\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "DsTyu4cwzkOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_duplicates(directory):\n",
        "    hashes = defaultdict(list)\n",
        "    for fname in os.listdir(directory):\n",
        "        fpath = os.path.join(directory, fname)\n",
        "        img = Image.open(fpath)\n",
        "        h = imagehash.average_hash(img)\n",
        "        hashes[h].append(fpath)\n",
        "\n",
        "    # Print duplicates\n",
        "    for h, files in hashes.items():\n",
        "        if len(files) > 1:\n",
        "            print(\"Duplicates found:\", files)\n",
        "\n",
        "# Check for duplicates in Monet and Photo sets\n",
        "find_duplicates('/content/data/monet_jpg')\n",
        "find_duplicates('/content/data/photo_jpg')"
      ],
      "metadata": {
        "id": "8Giy3B_GKVpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Color/Mode Consistency**\n",
        "\n",
        "Make sure all images are in RGB mode. If any image is grayscale or has an alpha channel, convert it:"
      ],
      "metadata": {
        "id": "ghQUTXqMKZdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ensure_rgb(directory):\n",
        "    for fname in os.listdir(directory):\n",
        "        fpath = os.path.join(directory, fname)\n",
        "        img = Image.open(fpath)\n",
        "        if img.mode != 'RGB':\n",
        "            img = img.convert('RGB')\n",
        "            img.save(fpath)\n",
        "\n",
        "ensure_rgb('/content/data/monet_jpg')\n",
        "ensure_rgb('/content/data/photo_jpg')"
      ],
      "metadata": {
        "id": "ELaaB1i5KZsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "yLGkuWksLRkV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Splitting or Sampling the Data**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6YENDBovW1RU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalization**\n",
        "\n",
        "It is standard practice to normalize pixel values for GAN training. CycleGAN, for example, often normalizes images to the range [-1, 1].\n",
        "\n",
        "To achieve this normalization, you’ll likely do it on-the-fly with TensorFlow or PyTorch during dataset creation. For example, with TensorFlow:"
      ],
      "metadata": {
        "id": "zm_XyrigW-tK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def load_and_preprocess_image(img_path):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    # Resize if not already done\n",
        "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
        "    # Normalize from [0,255] to [-1,1]\n",
        "    img = (img / 127.5) - 1\n",
        "    return img"
      ],
      "metadata": {
        "id": "WzcRojVyXG1n"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating TensorFlow Datasets**\n",
        "\n",
        "Generate TF Dataset objects for Monet and photo images. This makes it easier to batch, shuffle, and prefetch."
      ],
      "metadata": {
        "id": "h9V2SeIwXKLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "monet_paths = tf.data.Dataset.list_files('/content/data/monet_jpg/*.jpg', shuffle=True)\n",
        "photo_paths = tf.data.Dataset.list_files('/content/data/photo_jpg/*.jpg', shuffle=True)\n",
        "\n",
        "monet_ds = monet_paths.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE).batch(1)\n",
        "photo_ds = photo_paths.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE).batch(1)\n",
        "\n",
        "# For CycleGAN training, often we zip these datasets so that each step you get a Monet and a photo image:\n",
        "dataset = tf.data.Dataset.zip((monet_ds, photo_ds)).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "vADA7z63XKT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Augmentation**\n",
        "\n",
        "Applying data augmentation can help the model generalize better."
      ],
      "metadata": {
        "id": "3kUuS7yTXTGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_image(img):\n",
        "    img = tf.image.random_flip_left_right(img)\n",
        "    # Add other augmentations if desired\n",
        "    return img\n",
        "\n",
        "# Apply augmentations\n",
        "monet_ds = monet_ds.map(augment_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "photo_ds = photo_ds.map(augment_image, num_parallel_calls=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "eHYXFdKzXTT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining Model Architecture\n",
        "\n",
        "\n",
        "I plan to compare two models designed for image-to-image translation: a conditional Generative Adversarial Network (cGAN)-based model and a CycleGAN. \tThe conditional GAN approach will be used as a baseline, and the CycleGAN will add cycle-consistency constraints at the cost of increased complexity to try to improve performance.\n",
        "\n",
        "\n",
        "**Model 1:** CUT (Contrastive Unpaired Translation) Architecture:\n",
        "-  A newer framework that avoids explicit cycle-consistency by adopting a patchwise contrastive loss, which encourages the generated image to retain similar local features as the source image.\n",
        "- Instead of enforcing that an image translated A → B → A returns to its original form, CUT focuses on aligning patches between the input and output images at a feature level using contrastive learning.\n",
        "- Components\n",
        "  - Single Generator: A single generator network that transforms images from domain A to domain B\n",
        "  - Single Discriminator: A standard discriminator to ensure that the generated images are plausible in the target domain.\n",
        "  - Contrastive Loss: Guides the generator to preserve content-specific details\n",
        "- Simpler conceptually than CycleGAN\n",
        "- Potential t0 ignore crucial details and produce images that are stylistically correct but lose the content structure.\n",
        "\n",
        "\n",
        "**Model 2:** CycleGAN:\n",
        "- A well-established framework for unpaired image-to-image translation.\n",
        "- Designed for unpaired image-to-image translation\n",
        "- Does not require paired examples of source and target images\n",
        "- Uses two generators and two discriminators\n",
        "  -  One generator (G) translates photos to Monet paintings, and the other (F) translates Monet paintings back to photos.\n",
        "  - One discriminator determines if an image is a real Monet painting or a generated Monet-style image, the other determines if an image is a real photo or a generated photo.\n",
        "- More complex training setup (two generators, two discriminators, and additional loss terms)."
      ],
      "metadata": {
        "id": "JYMilRgBRqW-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building  Models"
      ],
      "metadata": {
        "id": "kdB0EIp8XtgU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CUT (Contrastive Unpaired Translation) Architecture\n",
        "\n",
        "**Configurations**\n",
        "\n"
      ],
      "metadata": {
        "id": "-IIeBCQtYAK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install tensorflow-addons\n",
        "!pip install --upgrade tensorflow tensorflow-addons keras"
      ],
      "metadata": {
        "id": "h-qBueFwy3Ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "\n",
        "N_RES = 9\n",
        "TAU = 0.07\n",
        "LR = 2e-4\n",
        "BETA_1 = 0.5\n",
        "EPOCHS = 10\n",
        "STEPS_PER_EPOCH = 1000\n",
        "NCE_LAYERS = [\"re_lu\", \"re_lu_1\", \"re_lu_2\"]"
      ],
      "metadata": {
        "id": "1fW3dQ5l5iSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Components**"
      ],
      "metadata": {
        "id": "-wpJEboq5qRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resnet_block(x, filters):\n",
        "    \"\"\"A single ResNet block as used in image-to-image translation models.\"\"\"\n",
        "    init = tf.random_normal_initializer(0., 0.02)\n",
        "    # Use Instance Normalization as is common in style-transfer tasks\n",
        "    # If unavailable, can revert to BatchNormalization\n",
        "    y = tf.keras.layers.Conv2D(filters, 3, padding='same', kernel_initializer=init)(x)\n",
        "    y = tfa.layers.InstanceNormalization()(y)\n",
        "    y = tf.keras.layers.ReLU()(y)\n",
        "    y = tf.keras.layers.Conv2D(filters, 3, padding='same', kernel_initializer=init)(y)\n",
        "    y = tfa.layers.InstanceNormalization()(y)\n",
        "    return tf.keras.layers.add([x, y])  # residual connection\n",
        "\n",
        "def build_generator(input_shape=(256,256,3), n_res=9):\n",
        "    init = tf.random_normal_initializer(0., 0.02)\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "    # Downsampling\n",
        "    x = tf.keras.layers.Conv2D(64, 7, padding='same', kernel_initializer=init)(inputs)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(128, 3, strides=2, padding='same', kernel_initializer=init)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    x = tf.keras.layers.Conv2D(256, 3, strides=2, padding='same', kernel_initializer=init)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "\n",
        "    # Residual blocks\n",
        "    for _ in range(n_res):\n",
        "        x = resnet_block(x, 256)\n",
        "\n",
        "    # Upsampling\n",
        "    x = tf.keras.layers.Conv2DTranspose(128, 3, strides=2, padding='same', kernel_initializer=init)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2DTranspose(64, 3, strides=2, padding='same', kernel_initializer=init)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(3, 7, padding='same', kernel_initializer=init, activation='tanh')(x)\n",
        "\n",
        "    return tf.keras.Model(inputs, x, name='cut_generator')\n",
        "\n",
        "\n",
        "def build_discriminator(input_shape=(256,256,3)):\n",
        "    \"\"\"Builds a PatchGAN discriminator.\"\"\"\n",
        "    init = tf.random_normal_initializer(0., 0.02)\n",
        "    inp = tf.keras.Input(shape=input_shape, name='dis_input')\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(64,4,strides=2,padding='same',kernel_initializer=init)(inp)\n",
        "    x = tf.keras.layers.LeakyReLU()(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(128,4,strides=2,padding='same',kernel_initializer=init)(x)\n",
        "    x = tfa.layers.InstanceNormalization()(x)\n",
        "    x = tf.keras.layers.LeakyReLU()(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(256,4,strides=2,padding='same',kernel_initializer=init)(x)\n",
        "    x = tfa.layers.InstanceNormalization()(x)\n",
        "    x = tf.keras.layers.LeakyReLU()(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(512,4,strides=1,padding='same',kernel_initializer=init)(x)\n",
        "    x = tfa.layers.InstanceNormalization()(x)\n",
        "    x = tf.keras.layers.LeakyReLU()(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(1,4,strides=1,padding='same',kernel_initializer=init)(x)\n",
        "    return tf.keras.Model(inp, x, name='cut_discriminator')"
      ],
      "metadata": {
        "id": "wsR3CMeKXs4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss Function**\n",
        "\n",
        "The CUT model uses PatchNCE (Contrastive) Loss. There is no function for this in tensorflow, so I have build a custom function modeled off the method used in PyTorch."
      ],
      "metadata": {
        "id": "ZqbAHSJOomBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "def gan_loss(pred_fake):\n",
        "    \"\"\"GAN loss for generator: tries to make pred_fake close to 1.\"\"\"\n",
        "    return mse(tf.ones_like(pred_fake), pred_fake)\n",
        "\n",
        "def gan_loss_discriminator(pred_real, pred_fake):\n",
        "    \"\"\"Discriminator loss: real close to 1, fake close to 0.\"\"\"\n",
        "    real_loss = mse(tf.ones_like(pred_real), pred_real)\n",
        "    fake_loss = mse(tf.zeros_like(pred_fake), pred_fake)\n",
        "    return 0.5 * (real_loss + fake_loss)\n",
        "\n",
        "def nce_loss(features_src, features_tgt, tau=TAU):\n",
        "    \"\"\"PatchNCE loss for one pair of feature maps (N,C,H,W).\"\"\"\n",
        "    N, C, H, W = tf.unstack(tf.shape(features_src))\n",
        "    features_src = tf.cast(features_src, tf.float32)\n",
        "    features_tgt = tf.cast(features_tgt, tf.float32)\n",
        "\n",
        "    # [N,C,H,W] -> [N,H,W,C]\n",
        "    features_src = tf.transpose(features_src, [0, 2, 3, 1])\n",
        "    features_tgt = tf.transpose(features_tgt, [0, 2, 3, 1])\n",
        "\n",
        "    # Flatten H*W patches\n",
        "    features_src = tf.reshape(features_src, [N * H * W, C])\n",
        "    features_tgt = tf.reshape(features_tgt, [N * H * W, C])\n",
        "\n",
        "    # Normalize\n",
        "    features_src = tf.nn.l2_normalize(features_src, axis=1)\n",
        "    features_tgt = tf.nn.l2_normalize(features_tgt, axis=1)\n",
        "\n",
        "    # Similarity\n",
        "    similarity = tf.matmul(features_src, features_tgt, transpose_b=True)\n",
        "    similarity = similarity / tau\n",
        "\n",
        "    num_patches = N * H * W\n",
        "    labels = tf.range(num_patches)\n",
        "\n",
        "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels, similarity)\n",
        "    return tf.reduce_mean(loss)\n"
      ],
      "metadata": {
        "id": "pjLWMjC8oldW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Extraction**"
      ],
      "metadata": {
        "id": "cL7q0yCcwYTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_feature_extractor(base_model, layer_names):\n",
        "    \"\"\"Extract intermediate features for NCE loss.\"\"\"\n",
        "    outputs = [base_model.get_layer(name).output for name in layer_names]\n",
        "    return tf.keras.Model(inputs=base_model.input, outputs=outputs, name='feature_extractor')\n"
      ],
      "metadata": {
        "id": "C0FLFnin55JR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build Models and Setup**"
      ],
      "metadata": {
        "id": "_I7Nw3hI6DO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator_CUT = build_generator()\n",
        "discriminator_CUT = build_discriminator()\n",
        "feature_extractor = create_feature_extractor(generator_CUT, NCE_LAYERS)\n",
        "\n",
        "gen_optimizer_CUT = tf.keras.optimizers.Adam(LR, beta_1=BETA_1)\n",
        "disc_optimizer_CUT = tf.keras.optimizers.Adam(LR, beta_1=BETA_1)\n",
        "\n",
        "# Setup TensorBoard logging\n",
        "log_dir = \"logs/CUT\"\n",
        "train_writer = tf.summary.create_file_writer(log_dir)"
      ],
      "metadata": {
        "id": "pFIW_MVa6JQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Setup**"
      ],
      "metadata": {
        "id": "gJGRXhij6SqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def cut_train_step(real_photo, real_monet):\n",
        "    # You can add identity loss if desired, for better color preservation:\n",
        "    # identity_monet = generator_CUT(real_monet, training=True)\n",
        "    # identity_loss_val = tf.reduce_mean(tf.abs(real_monet - identity_monet)) * 10.0\n",
        "    # Then add identity_loss_val to total_g_loss if you want\n",
        "\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        fake_monet = generator_CUT(real_photo, training=True)\n",
        "        disc_real = discriminator_CUT(real_monet, training=True)\n",
        "        disc_fake = discriminator_CUT(fake_monet, training=True)\n",
        "\n",
        "        g_gan_loss = gan_loss(disc_fake)\n",
        "        d_loss = gan_loss_discriminator(disc_real, disc_fake)\n",
        "\n",
        "        features_photo = feature_extractor(real_photo, training=True)\n",
        "        features_fakemonet = feature_extractor(fake_monet, training=True)\n",
        "\n",
        "        # Compute NCE loss\n",
        "        nce = 0.0\n",
        "        for f_p, f_fm in zip(features_photo, features_fakemonet):\n",
        "            # Convert to N,C,H,W for NCE\n",
        "            f_p = tf.transpose(f_p, [0,3,1,2])\n",
        "            f_fm = tf.transpose(f_fm, [0,3,1,2])\n",
        "            nce += nce_loss(f_p, f_fm, tau=TAU)\n",
        "        nce = nce / len(features_photo)\n",
        "\n",
        "        total_g_loss = g_gan_loss + nce  # + identity_loss_val if using\n",
        "\n",
        "    gen_grad = tape.gradient(total_g_loss, generator_CUT.trainable_variables)\n",
        "    disc_grad = tape.gradient(d_loss, discriminator_CUT.trainable_variables)\n",
        "\n",
        "    gen_optimizer_CUT.apply_gradients(zip(gen_grad, generator_CUT.trainable_variables))\n",
        "    disc_optimizer_CUT.apply_gradients(zip(disc_grad, discriminator_CUT.trainable_variables))\n",
        "\n",
        "    return g_gan_loss, nce, d_loss\n",
        "\n",
        "\n",
        "metrics_history_cut = {\n",
        "    'epoch': [],\n",
        "    'g_gan_loss': [],\n",
        "    'nce_loss': [],\n",
        "    'd_loss': []\n",
        "}\n"
      ],
      "metadata": {
        "id": "sXJbT1KoaefZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume `dataset` yields (photo, monet) pairs.\n",
        "# Ensure dataset is batched and provides [N,H,W,3] tensors.\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    g_gan_losses = []\n",
        "    nce_losses = []\n",
        "    d_losses = []\n",
        "\n",
        "    for step, (photo, monet) in enumerate(dataset.take(STEPS_PER_EPOCH)):\n",
        "        g_loss_val, nce_val, d_loss_val = cut_train_step(photo, monet)\n",
        "        g_gan_losses.append(g_loss_val.numpy())\n",
        "        nce_losses.append(nce_val.numpy())\n",
        "        d_losses.append(d_loss_val.numpy())\n",
        "\n",
        "    avg_g_gan_loss = np.mean(g_gan_losses)\n",
        "    avg_nce_loss = np.mean(nce_losses)\n",
        "    avg_d_loss = np.mean(d_losses)\n",
        "\n",
        "    # Log metrics\n",
        "    metrics_history_cut['epoch'].append(epoch)\n",
        "    metrics_history_cut['g_gan_loss'].append(avg_g_gan_loss)\n",
        "    metrics_history_cut['nce_loss'].append(avg_nce_loss)\n",
        "    metrics_history_cut['d_loss'].append(avg_d_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch}/{EPOCHS} completed: \"\n",
        "          f\"g_gan_loss={avg_g_gan_loss:.4f}, nce_loss={avg_nce_loss:.4f}, d_loss={avg_d_loss:.4f}\")\n",
        "\n",
        "    # Write to TensorBoard\n",
        "    with train_writer.as_default():\n",
        "        tf.summary.scalar('g_gan_loss', avg_g_gan_loss, step=epoch)\n",
        "        tf.summary.scalar('nce_loss', avg_nce_loss, step=epoch)\n",
        "        tf.summary.scalar('d_loss', avg_d_loss, step=epoch)\n",
        "\n",
        "df_cut_metrics = pd.DataFrame(metrics_history_cut)\n",
        "print(df_cut_metrics.head())"
      ],
      "metadata": {
        "id": "XGxOOQZAlvXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_metrics_cut = pd.DataFrame(metrics_history)\n",
        "df_metrics_cycleGAN.head()"
      ],
      "metadata": {
        "id": "aW6K3ekFqdNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CycleGAN Architecture\n",
        "CycleGAN requires two generators and two discriminators, as well as cycle-consistency and identity losses."
      ],
      "metadata": {
        "id": "6WCNKn8WYVYZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generators**\n",
        "\n",
        " I am using a ResNet-based generator here."
      ],
      "metadata": {
        "id": "3nWHAYz4Yj9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model"
      ],
      "metadata": {
        "id": "HkPapWJ0_BIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def residual_block(x, filters=256):\n",
        "    init = tf.random_normal_initializer(0., 0.02)\n",
        "    y = layers.Conv2D(filters, 3, padding='same', kernel_initializer=init)(x)\n",
        "    y = layers.BatchNormalization()(y)\n",
        "    y = layers.ReLU()(y)\n",
        "    y = layers.Conv2D(filters, 3, padding='same', kernel_initializer=init)(y)\n",
        "    y = layers.BatchNormalization()(y)\n",
        "    return layers.add([x, y])\n",
        "\n",
        "def cyclegan_generator(input_shape=(256,256,3), num_res_blocks=6):\n",
        "    init = tf.random_normal_initializer(0., 0.02)\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Downsampling\n",
        "    x = layers.Conv2D(64, 7, padding='same', kernel_initializer=init)(inputs)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.Conv2D(128, 3, strides=2, padding='same', kernel_initializer=init)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.Conv2D(256, 3, strides=2, padding='same', kernel_initializer=init)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "\n",
        "    # Residual blocks\n",
        "    for _ in range(num_res_blocks):\n",
        "        x = residual_block(x, 256)\n",
        "\n",
        "    # Upsampling\n",
        "    x = layers.Conv2DTranspose(128, 3, strides=2, padding='same', kernel_initializer=init)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "\n",
        "    x = layers.Conv2DTranspose(64, 3, strides=2, padding='same', kernel_initializer=init)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "\n",
        "    x = layers.Conv2D(3, 7, padding='same', kernel_initializer=init, activation='tanh')(x)\n",
        "\n",
        "    return Model(inputs, x, name='cyclegan_generator')"
      ],
      "metadata": {
        "id": "4hMtN_CPYc-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Discriminators**\n",
        "\n",
        "CycleGAN discriminators are simpler PatchGAN discriminators, similar to the cGAN discriminator but without conditional concatenation. They only take a single image as input and classify patches as real or fake."
      ],
      "metadata": {
        "id": "I7ygDZ_aYb0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cyclegan_discriminator(input_shape=(256,256,3)):\n",
        "    init = tf.random_normal_initializer(0., 0.02)\n",
        "    inp = layers.Input(shape=input_shape, name='input_image')\n",
        "\n",
        "    x = layers.Conv2D(64,4,strides=2,padding='same',kernel_initializer=init)(inp)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "\n",
        "    x = layers.Conv2D(128,4,strides=2,padding='same',kernel_initializer=init)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "\n",
        "    x = layers.Conv2D(256,4,strides=2,padding='same',kernel_initializer=init)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "\n",
        "    x = layers.Conv2D(512,4,strides=1,padding='same',kernel_initializer=init)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "\n",
        "    x = layers.Conv2D(1,4,strides=1,padding='same',kernel_initializer=init)(x)\n",
        "    return Model(inp, x, name='cyclegan_discriminator')"
      ],
      "metadata": {
        "id": "93A2EvMyY7jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining Loss Functions**\n"
      ],
      "metadata": {
        "id": "kjmF_oLMY65K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "mse_loss = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    # The generator tries to fool the discriminator, so we compare to \"ones\"\n",
        "    return mse_loss(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = mse_loss(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = mse_loss(tf.zeros_like(fake_output), fake_output)\n",
        "    return (real_loss + fake_loss) * 0.5\n",
        "\n",
        "# For cycleGAN:\n",
        "LAMBDA = 10.0\n",
        "def cycle_consistency_loss(real_image, cycled_image):\n",
        "    return tf.reduce_mean(tf.abs(real_image - cycled_image)) * LAMBDA\n",
        "\n",
        "def identity_loss(real_image, same_image):\n",
        "    return tf.reduce_mean(tf.abs(real_image - same_image)) * LAMBDA * 0.5\n",
        "\n",
        "l1_loss = tf.keras.losses.MeanAbsoluteError()\n",
        "\n",
        "def l1_reconstruction_loss(real_image, generated_image):\n",
        "    return l1_loss(real_image, generated_image)"
      ],
      "metadata": {
        "id": "9HMt-_tKaCQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build generators and Discriminators, and Define Optimizer**"
      ],
      "metadata": {
        "id": "VpOp0pyZpxmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "G = cyclegan_generator()   # Photo -> Monet\n",
        "F = cyclegan_generator()   # Monet -> Photo\n",
        "D_M = cyclegan_discriminator() # Distinguish Monet from generated Monet\n",
        "D_P = cyclegan_discriminator() # Distinguish Photos from generated Photos\n",
        "\n",
        "generator_g_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "generator_f_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "discriminator_m_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "discriminator_p_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n"
      ],
      "metadata": {
        "id": "IAgggFdbuDL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train Model**"
      ],
      "metadata": {
        "id": "JZ5YaWcqFur6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def cyclegan_train_step(real_monet, real_photo, G, F, D_M, D_P):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        # Generator forward passes\n",
        "        fake_monet = G(real_photo, training=True)\n",
        "        cycled_photo = F(fake_monet, training=True)\n",
        "\n",
        "        fake_photo = F(real_monet, training=True)\n",
        "        cycled_monet = G(fake_photo, training=True)\n",
        "\n",
        "        # Identity mapping (optional)\n",
        "        same_monet = G(real_monet, training=True)\n",
        "        same_photo = F(real_photo, training=True)\n",
        "\n",
        "        # Discriminator outputs\n",
        "        disc_real_monet = D_M(real_monet, training=True)\n",
        "        disc_fake_monet = D_M(fake_monet, training=True)\n",
        "\n",
        "        disc_real_photo = D_P(real_photo, training=True)\n",
        "        disc_fake_photo = D_P(fake_photo, training=True)\n",
        "\n",
        "        # Generator losses\n",
        "        gen_g_loss = generator_loss(disc_fake_monet)\n",
        "        gen_f_loss = generator_loss(disc_fake_photo)\n",
        "\n",
        "        total_cycle_loss = cycle_consistency_loss(real_photo, cycled_photo) + cycle_consistency_loss(real_monet, cycled_monet)\n",
        "        total_identity_loss = identity_loss(real_monet, same_monet) + identity_loss(real_photo, same_photo)\n",
        "\n",
        "        total_gen_g_loss = gen_g_loss + total_cycle_loss + total_identity_loss\n",
        "        total_gen_f_loss = gen_f_loss + total_cycle_loss + total_identity_loss\n",
        "\n",
        "        # Discriminator losses\n",
        "        disc_m_loss = discriminator_loss(disc_real_monet, disc_fake_monet)\n",
        "        disc_p_loss = discriminator_loss(disc_real_photo, disc_fake_photo)\n",
        "\n",
        "    # Compute gradients\n",
        "    generator_g_gradients = tape.gradient(total_gen_g_loss, G.trainable_variables)\n",
        "    generator_f_gradients = tape.gradient(total_gen_f_loss, F.trainable_variables)\n",
        "\n",
        "    discriminator_m_gradients = tape.gradient(disc_m_loss, D_M.trainable_variables)\n",
        "    discriminator_p_gradients = tape.gradient(disc_p_loss, D_P.trainable_variables)\n",
        "\n",
        "    # Apply gradients\n",
        "    generator_g_optimizer.apply_gradients(zip(generator_g_gradients, G.trainable_variables))\n",
        "    generator_f_optimizer.apply_gradients(zip(generator_f_gradients, F.trainable_variables))\n",
        "    discriminator_m_optimizer.apply_gradients(zip(discriminator_m_gradients, D_M.trainable_variables))\n",
        "    discriminator_p_optimizer.apply_gradients(zip(discriminator_p_gradients, D_P.trainable_variables))\n",
        "\n",
        "    return {\n",
        "        'gen_g_loss': total_gen_g_loss,\n",
        "        'gen_f_loss': total_gen_f_loss,\n",
        "        'disc_m_loss': disc_m_loss,\n",
        "        'disc_p_loss': disc_p_loss,\n",
        "        'cycle_loss': total_cycle_loss,\n",
        "        'identity_loss': total_identity_loss\n",
        "    }"
      ],
      "metadata": {
        "id": "UocEyEI1acUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Logging Metrics Each Epoch, to be used in future Evaluation**"
      ],
      "metadata": {
        "id": "V8VIxw9masLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_history = {\n",
        "    'epoch': [],\n",
        "    'gen_g_loss': [],\n",
        "    'gen_f_loss': [],\n",
        "    'disc_m_loss': [],\n",
        "    'disc_p_loss': [],\n",
        "    'cycle_loss': [],\n",
        "    'identity_loss': []\n",
        "}\n",
        "\n",
        "# Suppose we have a dataset of pairs for CycleGAN (real_monet, real_photo)\n",
        "EPOCHS = 10\n",
        "steps_per_epoch = 1000  # adjust based on data size\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    gen_g_losses = []\n",
        "    gen_f_losses = []\n",
        "    disc_m_losses = []\n",
        "    disc_p_losses = []\n",
        "    cycle_losses = []\n",
        "    identity_losses = []\n",
        "\n",
        "    for step, (real_monet, real_photo) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        results = cyclegan_train_step(real_monet, real_photo, G, F, D_M, D_P)\n",
        "        gen_g_losses.append(results['gen_g_loss'].numpy())\n",
        "        gen_f_losses.append(results['gen_f_loss'].numpy())\n",
        "        disc_m_losses.append(results['disc_m_loss'].numpy())\n",
        "        disc_p_losses.append(results['disc_p_loss'].numpy())\n",
        "        cycle_losses.append(results['cycle_loss'].numpy())\n",
        "        identity_losses.append(results['identity_loss'].numpy())\n",
        "\n",
        "    # Average the metrics over the epoch\n",
        "    metrics_history['epoch'].append(epoch)\n",
        "    metrics_history['gen_g_loss'].append(sum(gen_g_losses)/len(gen_g_losses))\n",
        "    metrics_history['gen_f_loss'].append(sum(gen_f_losses)/len(gen_f_losses))\n",
        "    metrics_history['disc_m_loss'].append(sum(disc_m_losses)/len(disc_m_losses))\n",
        "    metrics_history['disc_p_loss'].append(sum(disc_p_losses)/len(disc_p_losses))\n",
        "    metrics_history['cycle_loss'].append(sum(cycle_losses)/len(cycle_losses))\n",
        "    metrics_history['identity_loss'].append(sum(identity_losses)/len(identity_losses))\n",
        "\n",
        "    print(f\"Epoch {epoch}/{EPOCHS} completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZtcdczKasab",
        "outputId": "9483076d-10f7-47b2-8276-d0b5d976441a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 completed.\n",
            "Epoch 2/10 completed.\n",
            "Epoch 3/10 completed.\n",
            "Epoch 4/10 completed.\n",
            "Epoch 5/10 completed.\n",
            "Epoch 6/10 completed.\n",
            "Epoch 7/10 completed.\n",
            "Epoch 8/10 completed.\n",
            "Epoch 9/10 completed.\n",
            "Epoch 10/10 completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator_G.save('generator_G.h5')   # Photo->Monet generator\n",
        "generator_F.save('generator_F.h5')   # Monet->Photo generator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "64dw65ToAc0-",
        "outputId": "8473e382-15d2-46f6-cc87-972da22e1ee1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'generator_G' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-9df8b41adad7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerator_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'generator_G.h5'\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Photo->Monet generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgenerator_F\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'generator_F.h5'\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Monet->Photo generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'generator_G' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation of the CycleGan Method**"
      ],
      "metadata": {
        "id": "8E6PCtEuAdwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_metrics_cycleGAN = pd.DataFrame(metrics_history)\n",
        "df_metrics_cycleGAN.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "uRgjv7A1a2eR",
        "outputId": "49d62c20-8850-474c-f470-2afe31b55b28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   epoch  gen_g_loss  gen_f_loss  disc_m_loss  disc_p_loss  cycle_loss  \\\n",
              "0      1    8.993119    8.923896     0.451812     0.433370    5.777522   \n",
              "1      2    6.866129    6.857996     0.262974     0.280320    4.436697   \n",
              "2      3    6.354539    6.311831     0.242731     0.248642    4.068135   \n",
              "3      4    6.131477    6.075729     0.243348     0.259131    3.895566   \n",
              "4      5    6.095644    6.027769     0.227133     0.247844    3.865579   \n",
              "\n",
              "   identity_loss  \n",
              "0       2.710005  \n",
              "1       2.034035  \n",
              "2       1.849852  \n",
              "3       1.776513  \n",
              "4       1.755112  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0a122535-f69e-4476-a496-9a5407a6bd69\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>epoch</th>\n",
              "      <th>gen_g_loss</th>\n",
              "      <th>gen_f_loss</th>\n",
              "      <th>disc_m_loss</th>\n",
              "      <th>disc_p_loss</th>\n",
              "      <th>cycle_loss</th>\n",
              "      <th>identity_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>8.993119</td>\n",
              "      <td>8.923896</td>\n",
              "      <td>0.451812</td>\n",
              "      <td>0.433370</td>\n",
              "      <td>5.777522</td>\n",
              "      <td>2.710005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>6.866129</td>\n",
              "      <td>6.857996</td>\n",
              "      <td>0.262974</td>\n",
              "      <td>0.280320</td>\n",
              "      <td>4.436697</td>\n",
              "      <td>2.034035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>6.354539</td>\n",
              "      <td>6.311831</td>\n",
              "      <td>0.242731</td>\n",
              "      <td>0.248642</td>\n",
              "      <td>4.068135</td>\n",
              "      <td>1.849852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>6.131477</td>\n",
              "      <td>6.075729</td>\n",
              "      <td>0.243348</td>\n",
              "      <td>0.259131</td>\n",
              "      <td>3.895566</td>\n",
              "      <td>1.776513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>6.095644</td>\n",
              "      <td>6.027769</td>\n",
              "      <td>0.227133</td>\n",
              "      <td>0.247844</td>\n",
              "      <td>3.865579</td>\n",
              "      <td>1.755112</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0a122535-f69e-4476-a496-9a5407a6bd69')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0a122535-f69e-4476-a496-9a5407a6bd69 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0a122535-f69e-4476-a496-9a5407a6bd69');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a5798e28-994a-427c-8605-eb78930e1c97\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a5798e28-994a-427c-8605-eb78930e1c97')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a5798e28-994a-427c-8605-eb78930e1c97 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_metrics_cycleGAN",
              "summary": "{\n  \"name\": \"df_metrics_cycleGAN\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"epoch\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 1,\n        \"max\": 10,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          9,\n          2,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gen_g_loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0751308936545159,\n        \"min\": 5.340840158462524,\n        \"max\": 8.993119314511617,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          5.340840158462524,\n          6.866128977139791,\n          5.961693340142568\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gen_f_loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0770292688014826,\n        \"min\": 5.28572256008784,\n        \"max\": 8.923895570437113,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          5.28572256008784,\n          6.857996145884196,\n          5.901519809563955\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"disc_m_loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07220144354282083,\n        \"min\": 0.21165547190854947,\n        \"max\": 0.45181236406167347,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.21673968484004338,\n          0.26297414883971215,\n          0.22180572153379519\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"disc_p_loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06092556730947908,\n        \"min\": 0.23027623909215134,\n        \"max\": 0.43336989656090735,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.23732366236547628,\n          0.28032011436919374,\n          0.23622442955772083\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cycle_loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7220415175317942,\n        \"min\": 3.337087033589681,\n        \"max\": 5.777522310415904,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          3.337087033589681,\n          4.436697138945262,\n          3.7722708225250243\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"identity_loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.35483711879388347,\n        \"min\": 1.5152062205473582,\n        \"max\": 2.710005462169647,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          1.5152062205473582,\n          2.0340347707271578,\n          1.7083660451571148\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Transformations on the test data to submit to kaggle competition for scoring."
      ],
      "metadata": {
        "id": "Rg9noZWnCwoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "test_photos_paths = glob.glob('/kaggle/input/gan-getting-started/test_photos/*.jpg')  # Adjust path as needed\n",
        "\n",
        "def load_and_preprocess_image(img_path):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, [256,256])\n",
        "    # Normalize as per training, for CycleGAN typically [-1,1]\n",
        "    img = (img / 127.5) - 1\n",
        "    return tf.expand_dims(img, 0)  # Add batch dimension"
      ],
      "metadata": {
        "id": "x9N9pH5xCOd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "os.makedirs('generated_images', exist_ok=True)\n",
        "\n",
        "for i, path in enumerate(test_photos_paths):\n",
        "    input_image = load_and_preprocess_image(path)\n",
        "    fake_monet = generator_G(input_image, training=False)\n",
        "    # De-normalize: from [-1,1] to [0,255]\n",
        "    fake_monet = (fake_monet[0].numpy() * 127.5 + 127.5).astype(np.uint8)\n",
        "    out_path = f'generated_images/image_{i}.jpg'\n",
        "    tf.keras.utils.save_img(out_path, fake_monet)"
      ],
      "metadata": {
        "id": "eL4-7qLlCQD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Examine Training Process**"
      ],
      "metadata": {
        "id": "BpxRDfxICNhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "# Plot generator losses\n",
        "plt.plot(df_metrics_cycleGAN['epoch'], df_metrics_cycleGAN['gen_g_loss'], label='gen_g_loss')\n",
        "plt.plot(df_metrics_cycleGAN['epoch'], df_metrics_cycleGAN['gen_f_loss'], label='gen_f_loss')\n",
        "\n",
        "# Plot discriminator losses\n",
        "plt.plot(df_metrics_cycleGAN['epoch'], df_metrics_cycleGAN['disc_m_loss'], label='disc_m_loss')\n",
        "plt.plot(df_metrics_cycleGAN['epoch'], df_metrics_cycleGAN['disc_p_loss'], label='disc_p_loss')\n",
        "\n",
        "# Plot cycle and identity losses\n",
        "plt.plot(df_metrics_cycleGAN['epoch'], df_metrics_cycleGAN['cycle_loss'], label='cycle_loss')\n",
        "plt.plot(df_metrics_cycleGAN['epoch'], df_metrics_cycleGAN['identity_loss'], label='identity_loss')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('CycleGAN Training Losses Over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eUcOrmsCDAWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "Qbhk4M_uX2_k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the CUT model seemed like a good, more straightforward approach, the training of the model was quite complx, requiring custom loss functions, and took significant time.\n",
        "\n",
        "As the performance was also slightly lower than the CylceGAN, I am moving forward with the CycleGAN approach. In this section I will perform hyperparameter tuning on the model."
      ],
      "metadata": {
        "id": "s5iRc5raAgXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_tuner as kt\n",
        "\n",
        "def build_model(hp):\n",
        "    # Choose LR from a range\n",
        "    lr = hp.Choice('learning_rate', [2e-4, 1e-4, 5e-5])\n",
        "    lam = hp.Choice('lambda_cycle', [5, 10, 15])\n",
        "\n",
        "    # Build generator discriminator with chosen hyperparameters\n",
        "    generator = build_generator(n_res=9)\n",
        "    discriminator = build_discriminator()\n",
        "\n",
        "    gen_optimizer = tf.keras.optimizers.Adam(lr, beta_1=0.5)\n",
        "    disc_optimizer = tf.keras.optimizers.Adam(lr, beta_1=0.5)\n",
        "\n",
        "    # Return a compile-like step or a model object. Since CycleGAN isn't a standard Keras compile scenario,\n",
        "    # you might need a custom training loop integrated here or wrap training in a lambda.\n",
        "\n",
        "    return (generator, discriminator, gen_optimizer, disc_optimizer, lam)\n",
        "\n",
        "def hypermodel(hp):\n",
        "    # Build and do partial training, return a metric (e.g., cycle_loss or FID)\n",
        "    generator, discriminator, gen_opt, disc_opt, lam = build_model(hp)\n",
        "    # Perform a short training session (e.g., 2-3 epochs) and compute a metric\n",
        "    fid_score = train_and_get_fid(generator, discriminator, gen_opt, disc_opt, lambda_cycle=lam, epochs=3)\n",
        "    return fid_score\n",
        "\n",
        "tuner = kt.BayesianOptimization(\n",
        "    hypermodel,\n",
        "    objective='val_loss',  # or a custom objective like negative FID\n",
        "    max_trials=10,\n",
        "    directory='my_tuner_dir',\n",
        "    project_name='cyclegan_tuning'\n",
        ")\n",
        "\n",
        "tuner.search()\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]"
      ],
      "metadata": {
        "id": "U_LkxhkQDXjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "O-CEXVXFD94j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Model"
      ],
      "metadata": {
        "id": "OoH6fbUVD2Rm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "tYLCjThuD6fG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train Model on Test Set**"
      ],
      "metadata": {
        "id": "eYSL9h54GfRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_photos_paths = glob.glob('/kaggle/input/gan-getting-started/test_photos/*.jpg')  # Adjust path as needed\n",
        "\n",
        "def load_and_preprocess_image(img_path):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, [256,256])\n",
        "    # Normalize as per training, for CycleGAN typically [-1,1]\n",
        "    img = (img / 127.5) - 1\n",
        "    return tf.expand_dims(img, 0)  # Add batch dimension"
      ],
      "metadata": {
        "id": "FDAX_6xOEUur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('generated_images_final', exist_ok=True)\n",
        "\n",
        "for i, path in enumerate(test_photos_paths):\n",
        "    input_image = load_and_preprocess_image(path)\n",
        "    fake_monet = generator_G(input_image, training=False)\n",
        "    # De-normalize: from [-1,1] to [0,255]\n",
        "    fake_monet = (fake_monet[0].numpy() * 127.5 + 127.5).astype(np.uint8)\n",
        "    out_path = f'generated_images/image_{i}.jpg'\n",
        "    tf.keras.utils.save_img(out_path, fake_monet)"
      ],
      "metadata": {
        "id": "Qz2-ss-sEXyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plotting Training Metrics**"
      ],
      "metadata": {
        "id": "dReen7IHGmK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Example: Plot generator and discriminator losses\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(df_metrics_cycleGAN['epoch'], df_metrics_cycleGAN['gen_g_loss'], label='gen_g_loss')\n",
        "plt.plot(df_metrics_cycleGAN['epoch'], df_metrics_cycleGAN['gen_f_loss'], label='gen_f_loss')\n",
        "plt.plot(df_metrics_cycleGAN['epoch'], df_metrics_cycleGAN['disc_m_loss'], label='disc_m_loss')\n",
        "plt.plot(df_metrics_cycleGAN['epoch'], df_metrics_cycleGAN['disc_p_loss'], label='disc_p_loss')\n",
        "plt.plot(df_metrics_cycleGAN['epoch'], df_metrics_cycleGAN['cycle_loss'], label='cycle_loss')\n",
        "plt.plot(df_metrics_cycleGAN['epoch'], df_metrics_cycleGAN['identity_loss'], label='identity_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Losses Over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tWYt0WkBGqV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretation"
      ],
      "metadata": {
        "id": "59i68XlvEAQl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visual Comparison of Generated Outputs**"
      ],
      "metadata": {
        "id": "B54Weiq5GZtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Suppose you have a trained generator: generator_G (Photo->Monet)\n",
        "# and a directory of test photos: /path/to/test_photos\n",
        "\n",
        "def load_and_preprocess_image(img_path):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, [256,256])\n",
        "    img = (img / 127.5) - 1  # Scale to [-1,1]\n",
        "    return tf.expand_dims(img, 0)  # Add batch dimension\n",
        "\n",
        "test_image_paths = [\"/path/to/test_photos/img1.jpg\",\n",
        "                    \"/path/to/test_photos/img2.jpg\",\n",
        "                    \"/path/to/test_photos/img3.jpg\"]\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "for i, path in enumerate(test_image_paths):\n",
        "    input_image = load_and_preprocess_image(path)\n",
        "    fake_monet = generator_G(input_image, training=False)\n",
        "    fake_monet = (fake_monet[0].numpy() * 127.5 + 127.5).astype(np.uint8)\n",
        "\n",
        "    # Original image\n",
        "    orig = tf.image.decode_jpeg(tf.io.read_file(path), channels=3)\n",
        "    orig = tf.image.resize(orig, [256,256]).numpy().astype(np.uint8)\n",
        "\n",
        "    # Plot\n",
        "    plt.subplot(len(test_image_paths), 2, 2*i + 1)\n",
        "    plt.imshow(orig)\n",
        "    plt.title(\"Original\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(len(test_image_paths), 2, 2*i + 2)\n",
        "    plt.imshow(fake_monet)\n",
        "    plt.title(\"Translated (Monet-style)\")\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lhp5AonvGZLA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}